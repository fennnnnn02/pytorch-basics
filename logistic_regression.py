# -*- coding: utf-8 -*-
"""Logistic Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EGUXZOASXbu1YjummwVJmoRWWKh0UaF2
"""

import numpy as np
import torch 
import torch.nn as nn
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from torch.autograd import Variable

torch.manual_seed(1)

X,y = load_breast_cancer(return_X_y=True)

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,stratify=y)

X_train = Variable(torch.from_numpy(X_train)).float()

y_train = Variable(torch.from_numpy(y_train)).float()
X_test = Variable(torch.from_numpy(X_test)).float()
y_test = Variable(torch.from_numpy(y_test)).float()

class LogisticRegression(nn.Module):
  def __init__(self,input_size):
    super().__init__()
    self.linear = nn.Linear(input_size,1)
  
  def forward(self,x):
    return torch.nn.functional.sigmoid(self.linear(x)).view(-1)


model = LogisticRegression(X_train.shape[1])

criterion = torch.nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters())

history = {'loss':[],'val':[]}

for _ in range(1000):
  loss = criterion(model(X_train),y_train)
  val = criterion(model(X_test),y_test)

  optimizer.zero_grad()
  loss.backward()
  optimizer.step()
  history['loss'].append(loss.item())
  history['val'].append(val.item())

print(history)

